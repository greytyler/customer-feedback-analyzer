# ğŸ§  AI Customer Feedback Analyzer (Sentiment Classifier)

<img src="./screenshots/home-dark.png">

This project implements an **end-to-end** **AI-powered Customer Feedback Analyzer** built with **AWS AI Services** and **custom ML workflows**.\
It classifies customer reviews into **Positive, Negative, or Mixed** using **Amazon Comprehend**, **custom-trained SageMaker
model**, **S3**, **CloudWatch**, and a custom **Streamlit UI**, intergrated with **Python (sklearn, pandas, seaborn, boto3)**


This project demonstrates cloud-native ML deployment while also leveraging local developments for **EDA**, **preprocessing** and **testing**
<br/>

##### ğŸ“Š Architecture diagram


<img src="./screenshots/architecture.jpg" width=500/>

------------------------------------------------------------------------

#### ğŸ“‚ Project Structure
##### local VS Code:

    ai-feedback-analyzer/
    â”œâ”€â”€ data/
    â”‚   â”œâ”€â”€ reviews.csv                 # Raw dataset (downloaded from Kaggle)
    â”‚   â”œâ”€â”€ feedback_samples.csv        # Test dataset
    â”‚   â”œâ”€â”€ train_data.csv              # Training dataset (prepared for SageMaker)
    â”‚   â”œâ”€â”€ val.csv                     # Validation dataset
    â”‚
    â”œâ”€â”€ scripts/
    â”‚   â”œâ”€â”€ upload_to_s3.py             # Upload files to S3
    â”‚   â”œâ”€â”€ comprehend_analysis.py      # Sentiment & key phrase analysis with Amazon Comprehend
    â”‚   â”œâ”€â”€ prepare_training_data.py    # Converts Comprehend results to train/val CSVs
    â”‚   â”œâ”€â”€ sagemaker_training.ipynb    # SageMaker training + deployment steps
    â”‚   â”œâ”€â”€ train.py                    # Training script for SageMaker
    â”‚   â”œâ”€â”€ inference.py                # Endpoint deployment script
    â”‚   â”œâ”€â”€ sagemaker_inference.py      # Test predictions via endpoint locally
    â”‚   â””â”€â”€ visualize_results.ipynb     # EDA + model results visualization (seaborn, matplotlib)
    â”‚
    â”œâ”€â”€ app/
    â”‚   â””â”€â”€ app.py                      # Streamlit frontend UI
    â”‚
    â”œâ”€â”€ requirements.txt                # Dependencies
    â””â”€â”€ README.md                       # Project Documentation

##### Sagemaker Notebook (AWS Console):
    sagemaker-notebook/
    â”œâ”€â”€ data/
    â”‚   â”œâ”€â”€ train_data.csv 
    â”‚   â”œâ”€â”€ val.csv
    â”‚
    â”œâ”€â”€ sagemaker_training.ipynb        #trains model, saves to S3
    â”œâ”€â”€ inference.py                    #Deploys endpoint
    â”œâ”€â”€train.py

##### S3 Buckets folder structure
- `S3://grey-customer-feedback-analyzer/input/`â†’ raw datasets (reviews.csv, feedback_samples.csv)
- `S3://grey-customer-feedback-analyzer/processed/`â†’ reviews-analysis.csv
- `S3://grey-customer-feedback-analyzer/training/data/`â†’ train_data.csv, val.csv  after model training
- `S3://grey-customer-feedback-analyzer/models/`â†’ trained model artifacts
- `S3://grey-customer-feedback-analyzer/results/`â†’ model_predictions.csv 
------------------------------------------------------------------------

#### ğŸ”‘ Key AWS Services Used

-   **Amazon S3** â†’ Storage for datasets, training data, and model
    artifacts\
-   **Amazon Comprehend** â†’ Initial sentiment & key phrase analysis\
-   **Amazon SageMaker** â†’ Model training, deployment, and endpoint
    hosting\
-   **Amazon CloudWatch** â†’ Logs for model inference and application
    monitoring\
-   **IAM Roles & Policies** â†’ Secure access between services\
-   **Boto3 SDK** â†’ Python scripts to interact with AWS services

------------------------------------------------------------------------

### ğŸ“ Project Workflow

#### 1. Data Collection & Preprocessing

-   Raw dataset (`reviews.csv`).
-   Performed **EDA (Exploratory Data Analysis)** and **data cleaning**
    in **local Jupyter Notebook**.
-   Saved cleaned dataset to `reviews.csv` and uploaded to **S3**.
<img src="./screenshots/local notebook eda.png" width=800>

------------------------------------------------------------------------

### 2. Data Ingestion (S3 + Boto3)

-   Used `upload_to_s3.py` to upload datasets (`reviews.csv`,
    `feedback_samples.csv`) to **S3 input folder**.\
-   Configured **IAM role & permissions** for secure access.

<img src="./screenshots/S3 bucket 2.jpg">

------------------------------------------------------------------------

### 3. Sentiment Analysis with Amazon Comprehend

-   Ran `comprehend_analysis.py` locally in VS Code.\
-   Used **Amazon Comprehend** to detect sentiment + extract key
    phrases.\
-   Saved results to `reviews_analysis.csv` in **S3** and locally.

<img src="./screenshots/vs code reviews.csv.png">
<img src="./screenshots/S3 reviews_analysis.csv.jpg">

------------------------------------------------------------------------

### 4. Preparing Training Data for SageMaker

-   Executed `prepare_training_data.py`.\
-   Mapped **Comprehend sentiment â†’ numeric labels**.\
-   Generated `train_data.csv` and `val.csv` for SageMaker training.
<img src="./screenshots/vs code training data.png" width=850>


------------------------------------------------------------------------

### 5. Model Training & Deployment in Amazon SageMaker

Inside **SageMaker Notebook Instance**:

1.  **Cell 1** â†’ Upload training data to S3 (`data/` folder).\
2.  **Cell 2** â†’ Run `train.py` â†’ Train model (e.g., Linear Learner,
    XGBoost), which pulls training data from S3.
    -   Saves trained model to **S3 bucket models folder**.\
3.  **Cell 3** â†’ Run `inference.py` â†’ Deploy model as a **real-time
    endpoint** and automatically model in new bucket.
    
_Upload training data to S3:_
<img src="./screenshots/notebook setup.png" width=850/>
_Train model:_
<img src="./screenshots/notebook training complete.png" width=850/>
_Deploy model:_
<img src="./screenshots/notebook endpoint deployed.png">
_Sagemaker saved model in S3 bucket_
<img src="./screenshots/S3 sagemaker model.jpg" width=850>

------------------------------------------------------------------------

### 6. Model Testing & Inference (optional, for local)

-   Locally tested endpoint using `sagemaker_inference.py`
-   Used `feedback_samples.csv` for predictions.\
-   Saved results locally & in **S3**.

<img src ="./screenshots/S3 model_predictions.csv.jpg" width=850>

------------------------------------------------------------------------

### 7. Streamlit Web Application

-   Built `app.py` using **Streamlit**.\
-   Features:\
    âœ… Upload full CSV of feedback for batch predictions\
    âœ… Enter single feedback text for real-time prediction\
    âœ… Interactive visualizations (bar charts, tables)\
    âœ… Stylish minimal UI with AI-themed background vibes\
    âœ… Results saved to S3 & CloudWatch logs


_Batch predictions_:
<p float="left">
<img src ="./screenshots/csv upload light.png" width="49%">
<img src ="./screenshots/app result light.png" width="49%">
</p>

_Single prediction_:
<img src ="./screenshots/single upload pos.png" width=850>


------------------------------------------------------------------------

### 8. Results Visualization (EDA & Model Comparison)

-   In `visualize_results.ipynb | sagemaker_training.ipynb cell 4` (SageMaker Notebook):
    -   Loaded predictions from **S3**.
    -   Visualized using **pandas, seaborn, matplotlib**.

<img src="./screenshots/notebook results.png" width=850>

------------------------------------------------------------------------

### 9. CloudWatch Integration

-   Model inference logs automatically stored in **CloudWatch**.\
-   Monitored request/response logs for debugging & auditing.

<img src="./screenshots/cloudwatch logs.png" width=900>

------------------------------------------------------------------------

### ğŸ§¼ Cleanup Checklist

To avoid costs:\
- \[ \] Stop & delete SageMaker notebook instance\
- \[ \] Delete deployed endpoint\
- \[ \] Remove unused S3 data and model artifacts\
- \[ \] Monitor billing in AWS Console

------------------------------------------------------------------------

### âš™ï¸ Setup Instructions

1.  Clone repo:

    ``` bash
    git clone https://github.com/your-username/ai-feedback-analyzer.git
    cd ai-feedback-analyzer
    ```

2.  Install dependencies:

    ``` bash
    pip install -r requirements.txt
    ```

3.  Configure AWS credentials (IAM user with S3, Comprehend, SageMaker,
    CloudWatch access):

    ``` bash
    aws configure
    ```

4.  Run scripts step by step (see workflow above).

5.  Launch Streamlit app:

    ``` bash
    streamlit run app/app.py
    ```

------------------------------------------------------------------------

## ğŸ’° Cost-Saving Tips

-   Use **AWS Free Tier** whenever possible.
-   Shut down SageMaker endpoints & notebooks when not in use.

-   Keep datasets small (few MBs).

------------------------------------------------------------------------

## ğŸ“Œ Highlights

-   End-to-end AI pipeline with **AWS + Python**\
-   Combines **Comprehend & Custom Model** for robust classification\
-   Real-time predictions via **SageMaker Endpoint**\
-   Clean, professional **Streamlit UI**\
-   Integrated monitoring with **CloudWatch**

------------------------------------------------------------------------

## ğŸ† Outcome

The system can **classify customer feedback** into **Positive, Negative,
or Mixed** with:\
- Automated ingestion â†’ analysis â†’ classification â†’ visualization\
- Cloud-native + cost-optimized architecture\
- A **project** I have enjoyed in learning more about AWS AI/ML services.

------------------------------------------------------------------------
##### ğŸ‘¨â€ğŸ’» Author

###### ğŸ¤— My GitHubâ€™s got commits. My blogâ€™s got charisma ğŸ˜

<a href="https://greystack.hashnode.dev" target="_blank">
  <svg width="220" height="30" viewBox="0 0 260 32" xmlns="http://www.w3.org/2000/svg">
    <defs>
      <!-- Gradient for badge background -->
      <linearGradient id="badgeGrad" x1="0%" y1="0%" x2="100%" y2="100%">
        <stop offset="0%" stop-color="#08627eff" />
        <stop offset="100%" stop-color="#3d056bff" />
      </linearGradient>
      <!-- Path for animated border -->
      <path id="borderPath" d="M8,0 H252 A8,8 0 0 1 260,8 V24 A8,8 0 0 1 252,32 H8 A8,8 0 0 1 0,24 V8 A8,8 0 0 1 8,0 Z" />
      <!-- Gradient for border stroke -->
      <linearGradient id="glowGrad" x1="0%" y1="0%" x2="100%" y2="0%">
        <stop offset="0%" stop-color="#b8f0ffff" />
        <stop offset="100%" stop-color="#6aff45ff" />
      </linearGradient>
    </defs>
    <!-- Gradient background -->
    <rect width="260" height="32" rx="8" fill="url(#badgeGrad)" />
    <!-- Text label -->
    <text x="130" y="21" text-anchor="middle" fill="#ffffff" font-family="Verdana" font-size="14">
      â˜ï¸ greystack - code, cloud, AIâš¡
    </text>
    <!-- Animated border stroke -->
    <path d="M8,0 H252 A8,8 0 0 1 260,8 V24 A8,8 0 0 1 252,32 H8 A8,8 0 0 1 0,24 V8 A8,8 0 0 1 8,0 Z"
          fill="none" stroke="url(#glowGrad)" stroke-width="3"
          stroke-dasharray="1000" stroke-dashoffset="0">
      <animate attributeName="stroke-dashoffset" values="1000;0" dur="3s" repeatCount="indefinite" />
    </path>
  </svg>
</a>


###### ğŸ“² Feel free to connect or drop feedback on [Aubrey T Dube LinkedIn](https://www.linkedin.com/in/aubrey-t-dube-194896288/),  [GitHub](https://github.com/greytyler) or [Email](mailto:aubreytdube19@gmail.com)


